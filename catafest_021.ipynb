{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "catafest_021.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPR6LVQYOAPWYJBGS9fJqkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/catafest/colab_google/blob/master/catafest_021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSqdIzGd5XQn"
      },
      "source": [
        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, see this https://arxiv.org/abs/1810.04805. \n",
        "\n",
        "I used transformers, see this video: https://www.youtube.com/watch?v=SZorAJ4I-sA and this: https://arxiv.org/pdf/2003.08271.pdf.\n",
        "\n",
        "vocab.txt can be downloaded from Google's BERT repository, see one example: https://github.com/microsoft/SDNet/blob/master/bert_vocab_files/bert-base-uncased-vocab.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsC7wHi2vUOD",
        "outputId": "d35b7e7a-3fcb-44a0-c5d1-e285256ef243"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq9xMHPxugu0"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JswSJiRiwBnM"
      },
      "source": [
        "You can use this model directly with a pipeline for masked language modeling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtG4gSTbvfMR",
        "outputId": "40b246f9-1610-45d7-ae38-85b5fb7896d4"
      },
      "source": [
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtEE5kc56YRp"
      },
      "source": [
        "[MASK], [CLS], [SEP] -  are artificial tokens that are respectively inserted used in *Next Sentence Prediction - NSP*. For example: [SEP] is the special token which separates the 2 sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxoFrwaWvk8k",
        "outputId": "c38683df-bae6-41bf-ff76-4a7d1bbda3a7"
      },
      "source": [
        "unmasker(\"Hello I'm a [MASK].\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.026759833097457886,\n",
              "  'sequence': \"hello i'm a friend.\",\n",
              "  'token': 2767,\n",
              "  'token_str': 'friend'},\n",
              " {'score': 0.020836783573031425,\n",
              "  'sequence': \"hello i'm a lawyer.\",\n",
              "  'token': 5160,\n",
              "  'token_str': 'lawyer'},\n",
              " {'score': 0.020397325977683067,\n",
              "  'sequence': \"hello i'm a doctor.\",\n",
              "  'token': 3460,\n",
              "  'token_str': 'doctor'},\n",
              " {'score': 0.017135562375187874,\n",
              "  'sequence': \"hello i'm a girl.\",\n",
              "  'token': 2611,\n",
              "  'token_str': 'girl'},\n",
              " {'score': 0.01617709919810295,\n",
              "  'sequence': \"hello i'm a stranger.\",\n",
              "  'token': 7985,\n",
              "  'token_str': 'stranger'}]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQDmIhjT3XaC",
        "outputId": "f1efb6ab-ba87-4db4-d9a1-ad69b1fd183e"
      },
      "source": [
        "unmasker(\"Tell this word: [MASK].\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.033313069492578506,\n",
              "  'sequence': 'tell this word : no.',\n",
              "  'token': 2053,\n",
              "  'token_str': 'no'},\n",
              " {'score': 0.018001610413193703,\n",
              "  'sequence': 'tell this word : go.',\n",
              "  'token': 2175,\n",
              "  'token_str': 'go'},\n",
              " {'score': 0.010956508107483387,\n",
              "  'sequence': 'tell this word : yes.',\n",
              "  'token': 2748,\n",
              "  'token_str': 'yes'},\n",
              " {'score': 0.009109552018344402,\n",
              "  'sequence': 'tell this word : good.',\n",
              "  'token': 2204,\n",
              "  'token_str': 'good'},\n",
              " {'score': 0.008192724548280239,\n",
              "  'sequence': 'tell this word : death.',\n",
              "  'token': 2331,\n",
              "  'token_str': 'death'}]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAnaOHfu4cAc",
        "outputId": "d68f53bf-c225-44ba-823c-bb33d0ad2207"
      },
      "source": [
        "unmasker(\"[CLS] One [MASK] [SEP] Two [SEP]\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.9190663695335388,\n",
              "  'sequence': 'one. two',\n",
              "  'token': 1012,\n",
              "  'token_str': '.'},\n",
              " {'score': 0.057839520275592804,\n",
              "  'sequence': 'one ; two',\n",
              "  'token': 1025,\n",
              "  'token_str': ';'},\n",
              " {'score': 0.01602681539952755,\n",
              "  'sequence': 'one! two',\n",
              "  'token': 999,\n",
              "  'token_str': '!'},\n",
              " {'score': 0.006792058702558279,\n",
              "  'sequence': 'one? two',\n",
              "  'token': 1029,\n",
              "  'token_str': '?'},\n",
              " {'score': 0.00022950586571823806,\n",
              "  'sequence': 'one | two',\n",
              "  'token': 1064,\n",
              "  'token_str': '|'}]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2rqB-ec44Q3"
      },
      "source": [
        "Next source code for classification head you are instantiating a pretrained model for a different task the warning is telling you that some weights were randomly initialized - which is normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L866ZeqswDEn"
      },
      "source": [
        "PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJjQ0MbpwLzO",
        "outputId": "7a668d99-c4c8-4d43-ba9b-53bf24b96edc"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UAcxHBNwZxX"
      },
      "source": [
        "TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy3zO0mAwaHr",
        "outputId": "2c7f6700-a0d5-404c-dbe2-ded434190af6"
      },
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "output = model(encoded_input)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S922fQ9i-ESY"
      },
      "source": [
        "sentiment-analysis for a ProTV news text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juDiFT_P9aiZ",
        "outputId": "77861195-232d-4c34-8da7-79f8c234e55e"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I found out what awaits the 139 Afghans who arrived in Romania. The ridiculous amount they will receive from the state.\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9735418558120728}]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC0Xqd-g-Imt"
      },
      "source": [
        "https://www.theverge.com/2021/9/11/22668790/google-one-adds-5tb-storage-plan-photos text sentiment-analysis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il0fQ4Jt77RT",
        "outputId": "924dc6e6-7900-4830-baa6-a014165fbd7f"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"After it ended free unlimited storage for Google Photos in June, many Google users had figure out how to store images and other data in the Google accounts. They could keep their Google account stored data under 15GB, or pay for a Google One plan. Options included a 100GB plan for $1.99 per month, a 200GB plan for $2.99 a month, a 2TB plan for $9.99 a month, or a plan with 10TB of storage for $49.99 per month. 20TB and 30TB plans are also available, for $99.99 and $149.99 per month, respectively.\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9966930150985718}]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SZZzW9i-ZKt"
      },
      "source": [
        "https://www.theverge.com/2021/9/11/22668790/google-one-adds-5tb-storage-plan-photos text sentiment-analysis with more info "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnYpmvnE84ga",
        "outputId": "b6708bbc-c96d-4044-f313-fd944149ecea"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"After it ended free unlimited storage for Google Photos in June, many Google users had figure out how to store images and other data in the Google accounts. They could keep their Google account stored data under 15GB, or pay for a Google One plan. Options included a 100GB plan for $1.99 per month, a 200GB plan for $2.99 a month, a 2TB plan for $9.99 a month, or a plan with 10TB of storage for $49.99 per month. 20TB and 30TB plans are also available, for $99.99 and $149.99 per month, respectively. Now Google’s introduced a middle option between 2TB and 10TB, as noticed by 9to5Google. The 5TB Google One plan costs $24.99 per month, a good (and less expensive) option for people who want a little more than 2TB but don’t quite need a plan with 10TB of storage or more.If you’re sure the 5TB plan will meet your needs, you can save a little money by prepaying for a year’s subscription; it will run you $249.99. Like the 2TB and 10TB plans, the 5TB plan also includes 10 percent off Google Store purchases, the option to add family members, access to Google experts, and a VPN for Android phones.\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9948194622993469}]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}